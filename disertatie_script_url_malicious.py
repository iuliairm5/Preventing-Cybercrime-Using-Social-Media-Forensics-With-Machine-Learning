# -*- coding: utf-8 -*-
"""Disertatie_script_URL_malicious.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14AY6GbhVfd-ttIQO9FGWUKGk-Zcu6J3u
"""

# EDA Packages
import pandas as pd
import numpy as np
import random


# Machine Learning Packages
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

#1) Load Url Data 
urls_data = pd.read_csv("bigger_url_list.csv")
type(urls_data)
urls_data.head()

#2) Preprocessing data
#Data Vectorization Using TfidVectorizer
#Create A tokenizer
#Split ,Remove Repetitions and "Com"

def makeTokens(f):
    tkns_BySlash = str(f.encode('utf-8')).split('/')	# make tokens after splitting by slash
    total_Tokens = []
    for i in tkns_BySlash:
        tokens = str(i).split('-')	# make tokens after splitting by dash
        tkns_ByDot = []
        for j in range(0,len(tokens)):
            temp_Tokens = str(tokens[j]).split('.')	# make tokens after splitting by dot
            tkns_ByDot = tkns_ByDot + temp_Tokens
        total_Tokens = total_Tokens + tokens + tkns_ByDot
    total_Tokens = list(set(total_Tokens))	#remove redundant tokens
    if 'com' in total_Tokens:
        total_Tokens.remove('com')	#removing .com since it occurs a lot of times and it should not be included in our features
    return total_Tokens

# Labels/outputs
y = urls_data["label"]

# Features/inputs
url_list = urls_data["url"]

# Using Default Tokenizer
#vectorizer = TfidfVectorizer()

# Using Custom Tokenizer
vectorizer = TfidfVectorizer(tokenizer=makeTokens)

# Store vectors into X variable as Our XFeatures
X = vectorizer.fit_transform(url_list)

#3) Split into training and testing dataset 80/20 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#4) # Model Building using logistic regression
logreg = LogisticRegression()	
logreg.fit(X_train, y_train)

# Accuracy of Our Model
#print("Accuracy ",logit.score(X_test, y_test))



from sklearn.metrics import accuracy_score,f1_score,classification_report,confusion_matrix
# !!! NEW !!

from sklearn.metrics import classification_report
# Make predictions on the test set
y_pred = logreg.predict(X_test)

score_acc = accuracy_score(y_test,y_pred)
score_f1 = f1_score(y_test,y_pred,average="macro")

# Print classification report
print(classification_report(y_test, y_pred))
print("Accuracy: ", score_acc*100,"%")
print("F1-Score (Macro) : ",score_f1*100,"%")

confusionMatrix = confusion_matrix(y_test,y_pred)
import matplotlib.pyplot as plt
                                 
figure, axis = plt.subplots (figsize=(5,5))
axis.matshow(confusionMatrix,cmap=plt.cm.Blues,alpha=0.3)
for i in range(confusionMatrix.shape[0]):
  for j in range(confusionMatrix.shape[1]):
    axis.text(x=j, y=i, s=confusionMatrix[i , j], va="center", ha="center")
plt.xlabel("Predicted Label", fontsize=14)
plt.ylabel("Actual Label", fontsize=14)
plt.title("Confusion Matrix", fontsize=16)
plt.show()


import joblib
joblib.dump(logreg, 'url_model.joblib')

import pickle

with open("vectorizer_url.pkl", "wb") as f:
    pickle.dump(vectorizer, f)

# Load the model from the file
model = joblib.load('url_model.joblib')


with open("vectorizer_url.pkl", "rb") as f:
    vectorizer = pickle.load(f)


#5) testing the model by predincting
X_predict = ["google.com/search=jcharistech",
"google.com/search=faizanahmad",
"pakistanifacebookforever.com/getpassword.php/", 
"www.radsport-voggel.de/wp-admin/includes/log.exe", 
"ahrenhei.without-transfer.ru/nethost.exe ",
"www.itidea.it/centroesteticosothys/img/_notes/gum.exe"]

X_predict = vectorizer.transform(X_predict)
New_predict = model.predict(X_predict)
print(New_predict)

X_predict1 = ["www.buyfakebillsonlinee.blogspot.com", 
"www.unitedairlineslogistics.com",
"www.stonehousedelivery.com",
"www.silkroadmeds-onlinepharmacy.com" ]

X_predict1 = vectorizer.transform(X_predict1)
New_predict1 = model.predict(X_predict1)
print(New_predict1)




#6) save the trained model ????
#from sklearn.linear_model import LogisticRegression
#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Dense
#import h5py
# Convert the model to a TensorFlow/Keras model
#keras_model = Sequential()
#keras_model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid', use_bias=False))
#keras_model.set_weights([logit.coef_.T])

# Save the model to a file in h5 format
#keras_model.save('maliciousURLs.h5')

#7) Using Default Tokenizer
#vectorizer = TfidfVectorizer()
# Store vectors into X variable as Our XFeatures
#X = vectorizer.fit_transform(url_list)
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Model Building

#logit = LogisticRegression()	#using logistic regression
#logit.fit(X_train, y_train)

# Accuracy of Our Model with our Custom Token
#print("Accuracy ",logit.score(X_test, y_test))